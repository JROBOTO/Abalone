{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Learning WORKING AIRSTRIKER new CNN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JROBOTO/Abalone/blob/master/Learning_WORKING_AIRSTRIKER_new_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "_SzYiZIjETHX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Step 1: Install the required dependencies"
      ]
    },
    {
      "metadata": {
        "id": "t6xccwU1E5fW",
        "colab_type": "code",
        "collapsed": true,
        "outputId": "400c5cfd-877f-45f8-f70c-ec4756a71def",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "cell_type": "code",
      "source": [
        "!apt-get install pkg-config lua5.1 build-essential libav-tools git\n",
        "\n",
        "# rendering guide from https://star-ai.github.io/Rendering-OpenAi-Gym-in-Colaboratory/\n",
        "# https://mikelyons.org/2018/05/25/Running-Retro-on-Google-Colab.html\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install cmake > /dev/null 2>&1\n",
        "!pip install --upgrade setuptools 2>&1\n",
        "!pip install ez_setup > /dev/null 2>&1\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rReading package lists... 0%\r\rReading package lists... 0%\r\rReading package lists... 0%\r\rReading package lists... 8%\r\rReading package lists... 8%\r\rReading package lists... 8%\r\rReading package lists... 8%\r\rReading package lists... 75%\r\rReading package lists... 75%\r\rReading package lists... 76%\r\rReading package lists... 76%\r\rReading package lists... 79%\r\rReading package lists... 81%\r\rReading package lists... 81%\r\rReading package lists... 82%\r\rReading package lists... 82%\r\rReading package lists... 90%\r\rReading package lists... 90%\r\rReading package lists... 90%\r\rReading package lists... 90%\r\rReading package lists... 90%\r\rReading package lists... 90%\r\rReading package lists... 90%\r\rReading package lists... 90%\r\rReading package lists... 92%\r\rReading package lists... 92%\r\rReading package lists... 92%\r\rReading package lists... 92%\r\rReading package lists... 93%\r\rReading package lists... 93%\r\rReading package lists... 93%\r\rReading package lists... 93%\r\rReading package lists... 94%\r\rReading package lists... 94%\r\rReading package lists... 94%\r\rReading package lists... 94%\r\rReading package lists... 94%\r\rReading package lists... 94%\r\rReading package lists... 98%\r\rReading package lists... 98%\r\rReading package lists... 98%\r\rReading package lists... 98%\r\rReading package lists... Done\r\n",
            "\rBuilding dependency tree... 0%\r\rBuilding dependency tree... 0%\r\rBuilding dependency tree... 50%\r\rBuilding dependency tree... 50%\r\rBuilding dependency tree       \r\n",
            "\rReading state information... 0%\r\rReading state information... 0%\r\rReading state information... Done\r\n",
            "Package libav-tools is not available, but is referred to by another package.\n",
            "This may mean that the package is missing, has been obsoleted, or\n",
            "is only available from another source\n",
            "However the following packages replace it:\n",
            "  ffmpeg\n",
            "\n",
            "E: Package 'libav-tools' has no installation candidate\n",
            "Requirement already up-to-date: setuptools in /usr/local/lib/python3.6/dist-packages (40.8.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sjjt35KP2r1h",
        "colab_type": "code",
        "collapsed": true,
        "outputId": "0d690702-9425-4510-e8b9-737dfe6851f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        }
      },
      "cell_type": "code",
      "source": [
        "# for retro install\n",
        "!pip install tqdm retrowrapper gym-retro\n",
        "!pip install -U git+git://github.com/frenchie4111/dumbrain.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.28.1)\n",
            "Collecting retrowrapper\n",
            "  Downloading https://files.pythonhosted.org/packages/b2/5a/0156f8b29cf1ee07b225fc8913df114119ac449b4ce91812739582d609e0/retrowrapper-0.3.0-py3-none-any.whl\n",
            "Collecting gym-retro\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/67/2b/bee76fbe439a8a600854fb41fafcfad7efa57d1f3107bbca48ac4a1387cd/gym_retro-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (162.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 162.0MB 213kB/s \n",
            "\u001b[?25hRequirement already satisfied: pyglet==1.*,>=1.3.2 in /usr/local/lib/python3.6/dist-packages (from gym-retro) (1.3.2)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (from gym-retro) (0.10.11)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet==1.*,>=1.3.2->gym-retro) (0.16.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym->gym-retro) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym->gym-retro) (1.14.6)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from gym->gym-retro) (2.18.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym->gym-retro) (1.11.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym->gym-retro) (2019.3.9)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym->gym-retro) (1.22)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym->gym-retro) (3.0.4)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym->gym-retro) (2.6)\n",
            "Installing collected packages: gym-retro, retrowrapper\n",
            "Successfully installed gym-retro-0.7.0 retrowrapper-0.3.0\n",
            "Collecting git+git://github.com/frenchie4111/dumbrain.git\n",
            "  Cloning git://github.com/frenchie4111/dumbrain.git to /tmp/pip-req-build-qr1spooi\n",
            "Building wheels for collected packages: dumbrain\n",
            "  Building wheel for dumbrain (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /tmp/pip-ephem-wheel-cache-pg13or07/wheels/50/8e/6f/47c68c95113aa8c02ac02bde75673ace7c3d3636842c75fcb6\n",
            "Successfully built dumbrain\n",
            "Installing collected packages: dumbrain\n",
            "Successfully installed dumbrain-0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7pBPgNCq3ET8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Step 2: Install your games"
      ]
    },
    {
      "metadata": {
        "id": "q313vhaDGFpC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Don't run this command unless you already own the games, otherwise you are pirating :)\n",
        "#!python -m dumbrain.rl.retro_contest.install_games http://aiml.mikelyons.org/datasets/sonic/Sonic%20Roms.zip "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LOP-MsIwRDF6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Step 3: Create an env using retrowrapper\n"
      ]
    },
    {
      "metadata": {
        "id": "94R5bFiyRFU3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import retro\n",
        "import retrowrapper\n",
        "# for rendering \n",
        "\n",
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40) #error only\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay\n",
        "#eg screen resolution 1400x900\n",
        "import matplotlib.pyplot as plt\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env\n",
        "\n",
        "#env = retro.make(game='SonicTheHedgehog-Genesis', state='GreenHillZone.Act2')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W5Outje8AKBF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Step 4: Play the game"
      ]
    },
    {
      "metadata": {
        "id": "OapH7FxzFPPr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import gym.spaces\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import numpy as np\n",
        "from skimage import transform\n",
        "from collections import deque\n",
        "import retro\n",
        "from skimage.color import rgb2grey\n",
        "from skimage.color import rgb2hsv\n",
        "import random\n",
        "import cv2\n",
        "\n",
        "\n",
        "'''\n",
        "Gym provides the ability to wrap controllers and override parent classes to control how actions \n",
        "and rewards are handled.  \n",
        "'''\n",
        "\n",
        "\n",
        "class ActionController(gym.ActionWrapper):\n",
        "\n",
        "    def __init__(self, environment):\n",
        "        super(ActionController, self).__init__(environment)\n",
        "\n",
        "        buttons = [\"B\", \"A\", \"MODE\", \"START\", \"UP\", \"DOWN\", \"LEFT\", \"RIGHT\", \"C\", \"Y\", \"X\", \"Z\"]\n",
        "        actions = [['LEFT'], ['RIGHT'], ['LEFT', 'B'], ['RIGHT', 'B'], ['B']]\n",
        "        # TODO test combo actions - ['LEFT', 'DOWN'] ['LEFT', 'DOWN'] ['LEFT', 'DOWN'] - ?\n",
        "\n",
        "        self._actions = []\n",
        "        for action in actions:\n",
        "            # Turn off all actions\n",
        "            arr = np.array([False] * 12)\n",
        "\n",
        "            for button in action:\n",
        "                arr[buttons.index(button)] = True\n",
        "            self._actions.append(arr)\n",
        "\n",
        "        # sets the number of spaces to pick an action from (7)\n",
        "        self.action_space = gym.spaces.Discrete(len(self._actions))\n",
        "\n",
        "    def action(self, a):\n",
        "        return self._actions[a].copy()\n",
        "\n",
        "\n",
        "class RewardScaler(gym.RewardWrapper):\n",
        "    \"\"\"\n",
        "    Scales rewards from environment to be more consumable.\n",
        "    TODO More adjustments can be made here\n",
        "    \"\"\"\n",
        "\n",
        "    def reward(self, reward):\n",
        "        return reward * 0.01\n",
        "   \n",
        "    \n",
        "#\n",
        "\n",
        "\"\"\"\n",
        "This is a basic memory management systems which uses deque to store experiences. When the max memory size it met, old\n",
        "memories are discarded and the new memory is added to the front.\n",
        "\n",
        "based off: https://github.com/udacity/deep-learning/blob/master/reinforcement/Q-learning-cart.ipynb\n",
        "\"\"\"\n",
        "\n",
        "class Memory():\n",
        "    def __init__(self, max_memory_size):\n",
        "        self.buffer = deque(maxlen=max_memory_size)\n",
        "\n",
        "    def add(self, experience):\n",
        "        self.buffer.append(experience)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        index = np.random.choice(np.arange(len(self.buffer)),\n",
        "                                 size=batch_size,\n",
        "                                 replace=False)\n",
        "\n",
        "        return [self.buffer[ii] for ii in index]\n",
        "\n",
        "    # to prevent from sampling empty memory we need to know the size\n",
        "    def get_size(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        " \n",
        "\n",
        "\n",
        "class DQN():\n",
        "   \n",
        "    def conv_net(self, inputs):\n",
        "        conv1 = keras.layers.Conv2D(\n",
        "            filters=32,\n",
        "            kernel_size=[5, 5],\n",
        "            padding=\"same\",\n",
        "            activation=tf.nn.relu)(inputs)\n",
        "        \n",
        "        \n",
        "        pool1 = keras.layers.MaxPool2D(pool_size=[2, 2], strides=2)(conv1)\n",
        "\n",
        "        conv2 = keras.layers.Conv2D(\n",
        "            filters=64,\n",
        "            kernel_size=[5, 5],\n",
        "            padding=\"same\",\n",
        "            activation=tf.nn.relu)(pool1)\n",
        "        \n",
        "        pool2 = keras.layers.MaxPool2D( pool_size=[2, 2], strides=2)(conv2)\n",
        "        \n",
        "        conv3 = keras.layers.Conv2D(\n",
        "            filters=128,\n",
        "            kernel_size=[5, 5],\n",
        "            padding=\"same\",\n",
        "            activation=tf.nn.relu)(pool2)\n",
        "        \n",
        "        pool3 = keras.layers.MaxPool2D( pool_size=[2, 2], strides=2)(conv3)\n",
        "        \n",
        "        #print(pool3.shape)\n",
        "        pool2_flat = tf.reshape(pool3, [-1, 13 * 10 * 128])\n",
        "        dense = keras.layers.Dense(units=1024, activation=tf.nn.relu)(pool2_flat,)\n",
        "        dropout = keras.layers.Dropout(rate=1-0.4)(dense)\n",
        "        # training=tf.estimator.ModeKeys.TRAIN\n",
        "\n",
        "        # Logits Layer\n",
        "        logits = keras.layers.Dense(units=5)(dropout)\n",
        "        \n",
        "        return logits\n",
        "\n",
        "    def __init__(self, state_size, action_size, learning_rate, name='DQN', ENABLE_FAST_VERSION=False):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        with tf.variable_scope(name, reuse=tf.AUTO_REUSE):\n",
        "            #shape of input = [batch, in_height, in_width, in_channels]\n",
        "            #shape of filter = [filter_height, filter_width, in_channels, out_channels]\n",
        "            # Creating placeholders\n",
        "            self.inputs = tf.placeholder(tf.float32, [None, *state_size], name=\"inputs\")\n",
        "            self.actions = tf.placeholder(tf.float32, [None, self.action_size], name=\"actions\")  # TODO replace # with action space\n",
        "            self.target_Q = tf.placeholder(tf.float32, [None], name=\"target\")\n",
        "\n",
        "            self.global_step = tf.Variable(0, name='global_step', trainable=True)\n",
        "            self.output = self.conv_net(self.inputs)\n",
        "\n",
        "            # Q is our predicted Q value.\n",
        "            self.Q = tf.reduce_sum(tf.multiply(self.actions, self.output))\n",
        "\n",
        "            # The loss is the difference between our predicted Q_values and the Q_target\n",
        "            self.loss = tf.reduce_mean(tf.square(self.target_Q - self.Q), name=\"loss\")\n",
        "            tf.summary.scalar(\"loss\", self.loss)\n",
        "            self.optimizer = tf.train.RMSPropOptimizer(self.learning_rate).minimize(self.loss, global_step=self.global_step)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y9Tpkvr4nV6U",
        "colab_type": "code",
        "outputId": "3713bf10-ceb9-4376-899d-81f2b0fcf9af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2400
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "TRAINING_ACTIVE = True\n",
        "RENDER_ACTIVE = False\n",
        "\n",
        "EPISODES = 20 # 5\n",
        "MAXTIMESTEPS = 40000  # TODO is this enough? this is close to 10 min\n",
        "BATCH_SIZE = 64  # TODO optimise? 64\n",
        "\n",
        "LEARNING_RATE = 0.0001\n",
        "\n",
        "DISCOUNT_FACTOR = 0.4 #0.99  # TODO 0.99? 0.4\n",
        "\n",
        "\n",
        "EPSILON_START = 0.3  # 0.4\n",
        "EPSILON_END = 0.01\n",
        "EPSILON_DECAY = 0.0001\n",
        "\n",
        "IMAGE_WIDTH = 110  # 120\n",
        "IMAGE_HEIGHT = 84  # 84\n",
        "FRAMES_STACKED = 8\n",
        "\n",
        "ACTION_SIZE = 5  # THIS GETS SET AGAIN AFTER ENVIRONMENT IS SET UP\n",
        "STATE_SIZE = [IMAGE_WIDTH, IMAGE_HEIGHT, FRAMES_STACKED]\n",
        "\n",
        "MEMORY_SIZE = 15000\n",
        "\n",
        "# Initialize deque with zero-images one array for each image\n",
        "stacked_frames = deque([np.zeros((IMAGE_WIDTH, IMAGE_HEIGHT), dtype=np.int) for i in range(FRAMES_STACKED)],\n",
        "                       maxlen=FRAMES_STACKED)\n",
        "\n",
        "\n",
        "# Stats\n",
        "old_score = 0\n",
        "old_lives = 3\n",
        "\n",
        "lengths = []\n",
        "pltRewards = []\n",
        "\n",
        "def main():\n",
        "    # GPU TESTING\n",
        "    # if tf.test.is_built_with_cuda():\n",
        "    #     print(\"The installed version of TensorFlow includes GPU support.\")\n",
        "    # else:\n",
        "    #     print(\"The installed version of TensorFlow does not include GPU support.\")\n",
        "\n",
        "    run_model()\n",
        "\n",
        "\n",
        "def preprocess_frame(frame, restart):\n",
        "    \"\"\"\n",
        "    This method reduces the problems space. First grey scaling the frame, normalising it and shrinking it\n",
        "    Then stacking frames to create the concept of motion.\n",
        "    \"\"\"\n",
        "    grey_image = rgb2grey(frame)\n",
        "    norm_image = grey_image / 255\n",
        "    resized_image = transform.resize(norm_image, [IMAGE_WIDTH, IMAGE_HEIGHT], order=1)\n",
        "\n",
        "    # Need to call the global definition\n",
        "    global stacked_frames\n",
        "\n",
        "    # stack frames - give sense of motion\n",
        "    if restart:\n",
        "        # clear the stack (zero out each frame for each frame saved)\n",
        "        stacked_frames = deque([np.zeros((IMAGE_WIDTH, IMAGE_HEIGHT), dtype=np.int) for i in range(FRAMES_STACKED)],\n",
        "                               maxlen=FRAMES_STACKED)\n",
        "\n",
        "        # Make a stack of the same position\n",
        "        for i in range(FRAMES_STACKED):\n",
        "            stacked_frames.append(resized_image)\n",
        "\n",
        "        # Stack the frames\n",
        "        stacked_state = np.stack(stacked_frames, axis=2)\n",
        "\n",
        "    else:\n",
        "        stacked_frames.append(resized_image)\n",
        "        stacked_state = np.stack(stacked_frames, axis=2)\n",
        "\n",
        "    return stacked_state, stacked_frames\n",
        "\n",
        "\n",
        "def get_action(state, actions, epsilon, session, network):\n",
        "    \"\"\"\n",
        "        This method returns the action to take in the next state\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO improve decay rate\n",
        "    # We need the exploration to go up when stagnation occurs.\n",
        "    epsilon = np.maximum(epsilon - EPSILON_DECAY, EPSILON_END)\n",
        "\n",
        "    if random.random() > 1 - epsilon:\n",
        "        # action = np.random.choice(actions)\n",
        "        #print(\"random action\")\n",
        "        choice = random.randint(0, len(actions) - 1)  # possible action space.\n",
        "    else:\n",
        "        #print(\"Best action\")\n",
        "        # Estimate the new Q values based on current state\n",
        "        Qs = session.run(network.output, feed_dict={\n",
        "            network.inputs: state.reshape((1, *state.shape))})  # TODO are these the right inputs for this? # todo 1\n",
        "        # exploit: take the best action from the DQL\n",
        "        choice = np.argmax(Qs)\n",
        "        # action = possible_actions[int(choice)] TODO using choice instead of action\n",
        "\n",
        "    return choice, epsilon\n",
        "\n",
        "\n",
        "def get_environment():\n",
        "    \"\"\"\n",
        "        This method sets up the environment to be used. This includes creating a custom\n",
        "        action controller as well as loading the game rom and altering reward systems.\n",
        "\n",
        "        Retro provides a baseline to be used to start adjusting the environment. this includes\n",
        "        wrapper tools made to allow backtracking in the environment.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create environment\n",
        "    # Level notes: It gets stuck at a point where it needs to go back stand on a rock and jump off the rock\n",
        "    # environment = retro.make(game='SonicTheHedgehog-Genesis', state='GreenHillZone.Act1')\n",
        "\n",
        "    # Level notes: It needs to jump on an enemy underwater to get higher\n",
        "    # environment = retro.make(game='SonicTheHedgehog-Genesis', state='LabyrinthZone.Act1')\n",
        "\n",
        "    # Level notes: It needs a spin to get up a spin path\n",
        "    # environment = retro.make(game='SonicTheHedgehog-Genesis', state='GreenHillZone.Act2')\n",
        "\n",
        "    # Level notes: It needs a spin to get up a spin path\n",
        "    environment = retro.make(game='Airstriker-Genesis')\n",
        "    \n",
        "    environment=wrap_env(environment)\n",
        "    \n",
        "    # set up action controller\n",
        "    environment = ActionController(environment)\n",
        "    possible_actions = np.array(np.identity(environment.action_space.n, dtype=np.int).tolist())\n",
        "\n",
        "    return environment, possible_actions\n",
        "\n",
        "\n",
        "def modify_reward(reward, info, steps):\n",
        "    global old_score\n",
        "    global old_lives\n",
        "\n",
        "    score = info['score']\n",
        "    gameover = info['gameover']\n",
        "    lives = info['lives']\n",
        "\n",
        "    # by using score we can encourage it to kill enemies - score and rings are different\n",
        "    if score > old_score:\n",
        "        old_score = score\n",
        "        reward += 20.0\n",
        "    \n",
        "    \n",
        "    if lives < old_lives:\n",
        "        old_lives = lives\n",
        "        reward -= 40.0\n",
        "\n",
        "    return reward*0.1\n",
        "\n",
        "\n",
        "\n",
        "def get_status(rewards_collection, episode, step, xposition):\n",
        "    # early episodes it should not worry about stuck status\n",
        "    if not (episode > 5):\n",
        "        return False\n",
        "\n",
        "    # TODO check if this is actually a decent guess\n",
        "    if np.average(rewards_collection[:100]) < 1:\n",
        "        # Need to keep track of if it is always getting stuck in the same place\n",
        "        stuck_position.append(episode, xposition)\n",
        "\n",
        "def reset_globals():\n",
        "    global old_score\n",
        "    global old_lives\n",
        "    \n",
        "    old_score = 0\n",
        "    old_lives = 3\n",
        "\n",
        "    \n",
        "def run_model():\n",
        "    # the 7 is the new amount of actions space but the environment isn't fully set up here.\n",
        "    network = DQN(STATE_SIZE, ACTION_SIZE, LEARNING_RATE)\n",
        "\n",
        "    # To save the model\n",
        "    saver = tf.train.Saver()\n",
        "\n",
        "    if TRAINING_ACTIVE:\n",
        "        with tf.Session() as sess:\n",
        "            # try keep restoring\n",
        "            # saver.restore(sess, \"./model.ckpt\")\n",
        "\n",
        "            memory = Memory(MEMORY_SIZE)\n",
        "\n",
        "            # Initialize the variables\n",
        "            sess.run(tf.global_variables_initializer())\n",
        "\n",
        "            # Set up environment to allow exploring backwards and an action controller\n",
        "            environment, possible_actions = get_environment()\n",
        "            \n",
        "            epsilon = EPSILON_START\n",
        "          \n",
        "            for episode in range(EPISODES):\n",
        "                print(\"EPISODE \", episode)\n",
        "                # Init\n",
        "                done = False\n",
        "                epLength = 0\n",
        "                step = 0\n",
        "                \n",
        "                reset_globals()\n",
        "\n",
        "                # Initialize the rewards of the episode\n",
        "                episode_rewards = []\n",
        "                # total_reward = np.sum(episode_rewards)\n",
        "\n",
        "                # first state\n",
        "                frames = environment.reset()\n",
        "                #print(\"observation: {}\".format(frames.shape))\n",
        "                state, stacked_frames = preprocess_frame(frames, True)\n",
        "\n",
        "                while not done:\n",
        "                    if RENDER_ACTIVE:\n",
        "                        environment.render()\n",
        "\n",
        "                    # TODO NOTE: A step should be the time to preform one action NOT a frame.\n",
        "                    # TODO stop jumping so much.\n",
        "                    #  If the actions are up right left right down right - all of this will be in the air\n",
        "                    # Predict the action to take and take it\n",
        "                    if (step%20==0):\n",
        "                        action, epsilon = get_action(state, possible_actions, epsilon, sess, network)\n",
        "                    \n",
        "                    \n",
        "                    # Take the action\n",
        "                    frames, reward, done, info = environment.step(action)\n",
        "\n",
        "                    # provide custom rewards based on the info data file - eg rewards for killing enemies\n",
        "                    #reward = modify_reward(reward, info, step)\n",
        "\n",
        "                    # Add the reward to total reward\n",
        "                    episode_rewards.append(reward)\n",
        "\n",
        "#                   stuck = get_status(episode_rewards, episode, step, info['x'])\n",
        "\n",
        "#                   if stuck:\n",
        "#                       print(\"stuck\")\n",
        "#                       step + 100\n",
        "\n",
        "                    # this is for debugging\n",
        "                    if (step % 100 == 0):\n",
        "                        #print(epsilon)\n",
        "                        #print(action)\n",
        "                        #print(episode_rewards[-9:])\n",
        "                        epLength += 1\n",
        "                        total_reward = np.sum(episode_rewards)\n",
        "                        print(\"Total rewards: \", total_reward)\n",
        "                        #print(step)\n",
        "                        \n",
        "\n",
        "                    # roughly 5 min of game play\n",
        "                    if step >= MAXTIMESTEPS:\n",
        "                        done = True\n",
        "                        \n",
        "\n",
        "                    if done:\n",
        "                        # If it dies we need to reset the step count\n",
        "                        # step = 0\n",
        "                        # the episode ends so go next state\n",
        "                        next_state = np.zeros((IMAGE_HEIGHT, IMAGE_WIDTH), dtype=np.int)\n",
        "                        next_state, stacked_frames = preprocess_frame(next_state, False)\n",
        "\n",
        "                        # Get the total reward of the episode\n",
        "                        total_reward = np.sum(episode_rewards)\n",
        "                        print(\"Total rewards: \", total_reward)\n",
        "                        print(\"Episode length \", epLength)\n",
        "                        lengths.append(epLength)\n",
        "                        pltRewards.append(total_reward)\n",
        "\n",
        "                        memory.add((state, action, reward, next_state, done))\n",
        "\n",
        "                    else:\n",
        "                        # Get the next state\n",
        "                        next_state, stacked_frames = preprocess_frame(frames, False)\n",
        "\n",
        "                        # Add experience to memory\n",
        "                        memory.add((state, action, reward, next_state, done))\n",
        "\n",
        "                        # st+1 is now our current state\n",
        "                        state = next_state\n",
        "\n",
        "                    # When we have added enough memories we can start to learn from them\n",
        "                    if memory.get_size() > BATCH_SIZE and (step % 100 == 0):  # TODO how many steps?\n",
        "                        #print(\"MEMSAMPLE\")\n",
        "                        # Obtain random mini-batch from memory\n",
        "                        batch = np.array(memory.sample(BATCH_SIZE))  # TODO min *(memory.get_size(), BATCH_SIZE)\n",
        "                        states_mb = np.array([each[0] for each in batch], ndmin=3) \n",
        "                        # TODO ValueError: Cannot feed value of shape (64,) for Tensor 'DQN/actions_:0',\n",
        "                        #  which has shape '(?, 3)'\n",
        "                        # actions_mb = np.array([each[1] for each in batch])\n",
        "                        actions_mb = np.zeros((BATCH_SIZE, environment.action_space.n))\n",
        "                        for i in range(BATCH_SIZE):\n",
        "                            actions_mb[action] = action\n",
        "                            # print(\"action_mb:\", actions_mb)\n",
        "\n",
        "                        rewards_mb = np.array([each[2] for each in batch])\n",
        "\n",
        "                        next_states_mb = np.array([each[3] for each in batch], ndmin=3)\n",
        "\n",
        "                        dones_mb = np.array([each[4] for each in batch])\n",
        "\n",
        "                        target_Qs_batch = []\n",
        "\n",
        "                        # Get Q values for next_state\n",
        "                        Qs_next_state = sess.run(network.output, feed_dict={network.inputs: next_states_mb})\n",
        "\n",
        "                        # Set Q_target = r if the episode ends at s+1, otherwise set Q_target = r + gamma*maxQ(s', a')\n",
        "                        for i in range(0, len(batch)):\n",
        "                            terminal = dones_mb[i]\n",
        "\n",
        "                            # If we are in a terminal state, only equals reward\n",
        "                            if terminal:\n",
        "                                target_Qs_batch.append(rewards_mb[i])\n",
        "\n",
        "                            else:\n",
        "                                target = rewards_mb[i] + DISCOUNT_FACTOR * np.max(Qs_next_state[i])\n",
        "                                target_Qs_batch.append(target)\n",
        "\n",
        "                        targets_mb = np.array([each for each in target_Qs_batch])\n",
        "\n",
        "                        loss, _ = sess.run([network.loss, network.optimizer],\n",
        "                                           feed_dict={network.inputs: states_mb,\n",
        "                                                      network.target_Q: targets_mb,\n",
        "                                                      network.actions: actions_mb})\n",
        "\n",
        "                    step += 1\n",
        "\n",
        "                if episode % 100 == 0:\n",
        "                    saver.save(sess, \"./model.ckpt\")\n",
        "                    print(\"Model Saved\")\n",
        "\n",
        "    else:\n",
        "        # Load the model\n",
        "        # play the game\n",
        "        with tf.Session() as sess:\n",
        "            saver.restore(sess, \"./model.ckpt\")\n",
        "            # TODO make a playing state\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "\n",
        "#if KeyboardInterrupt: \n",
        "#    done = True    \n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(EPISODES), lengths, linewidth=2.0)\n",
        "plt.xlabel(\"epsiodes\")\n",
        "plt.ylabel(\"length of episode\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(EPISODES), pltRewards, linewidth=2.0)\n",
        "plt.xlabel(\"epsiodes\")\n",
        "plt.ylabel(\"epsiode rewards\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "show_video()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EPISODE  0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
            "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Total rewards:  0.0\n",
            "Total rewards:  0.0\n",
            "Total rewards:  0.0\n",
            "Total rewards:  20.0\n",
            "Total rewards:  20.0\n",
            "Total rewards:  20.0\n",
            "Total rewards:  20.0\n",
            "Total rewards:  20.0\n",
            "Total rewards:  20.0\n",
            "Total rewards:  20.0\n",
            "Total rewards:  20.0\n",
            "Total rewards:  20.0\n",
            "Total rewards:  20.0\n",
            "Total rewards:  20.0\n",
            "Total rewards:  20.0\n",
            "Total rewards:  40.0\n",
            "Total rewards:  40.0\n",
            "Episode length  16\n",
            "Model Saved\n",
            "EPISODE  1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
            "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Total rewards:  0.0\n",
            "Total rewards:  0.0\n",
            "Total rewards:  0.0\n",
            "Total rewards:  20.0\n",
            "Total rewards:  20.0\n",
            "Total rewards:  20.0\n",
            "Total rewards:  20.0\n",
            "Total rewards:  20.0\n",
            "Total rewards:  20.0\n",
            "Total rewards:  20.0\n",
            "Total rewards:  20.0\n",
            "Total rewards:  20.0\n",
            "Total rewards:  20.0\n",
            "Total rewards:  40.0\n",
            "Total rewards:  40.0\n",
            "Episode length  14\n",
            "EPISODE  2\n",
            "Total rewards:  0.0\n",
            "Total rewards:  0.0\n",
            "Total rewards:  0.0\n",
            "Total rewards:  0.0\n",
            "Total rewards:  20.0\n",
            "Total rewards:  20.0\n",
            "Total rewards:  20.0\n",
            "Total rewards:  20.0\n",
            "Total rewards:  40.0\n",
            "Total rewards:  40.0\n",
            "Total rewards:  40.0\n",
            "Total rewards:  40.0\n",
            "Total rewards:  60.0\n",
            "Total rewards:  60.0\n",
            "Total rewards:  60.0\n",
            "Total rewards:  60.0\n",
            "Total rewards:  80.0\n",
            "Total rewards:  80.0\n",
            "Total rewards:  80.0\n",
            "Total rewards:  80.0\n",
            "Total rewards:  80.0\n",
            "Total rewards:  80.0\n",
            "Total rewards:  100.0\n",
            "Total rewards:  100.0\n",
            "Episode length  23\n",
            "EPISODE  3\n",
            "Total rewards:  0.0\n",
            "Total rewards:  0.0\n",
            "Total rewards:  0.0\n",
            "Total rewards:  0.0\n",
            "Total rewards:  20.0\n",
            "Total rewards:  20.0\n",
            "Total rewards:  20.0\n",
            "Total rewards:  20.0\n",
            "Total rewards:  20.0\n",
            "Total rewards:  20.0\n",
            "Total rewards:  20.0\n",
            "Total rewards:  20.0\n",
            "Total rewards:  20.0\n",
            "Total rewards:  20.0\n",
            "Total rewards:  20.0\n",
            "Total rewards:  20.0\n",
            "Total rewards:  20.0\n",
            "Total rewards:  20.0\n",
            "Episode length  17\n",
            "EPISODE  4\n",
            "Total rewards:  0.0\n",
            "Total rewards:  0.0\n",
            "Total rewards:  0.0\n",
            "Total rewards:  0.0\n",
            "Total rewards:  0.0\n",
            "Total rewards:  0.0\n",
            "Total rewards:  0.0\n",
            "Total rewards:  0.0\n",
            "Total rewards:  0.0\n",
            "Total rewards:  0.0\n",
            "Total rewards:  0.0\n",
            "Total rewards:  0.0\n",
            "Total rewards:  0.0\n",
            "Total rewards:  0.0\n",
            "Total rewards:  0.0\n",
            "Total rewards:  0.0\n",
            "Episode length  15\n",
            "EPISODE  5\n",
            "Total rewards:  0.0\n",
            "Total rewards:  0.0\n",
            "Total rewards:  0.0\n",
            "Total rewards:  0.0\n",
            "Total rewards:  0.0\n",
            "Total rewards:  0.0\n",
            "Total rewards:  20.0\n",
            "Total rewards:  20.0\n",
            "Total rewards:  20.0\n",
            "Total rewards:  20.0\n",
            "Total rewards:  20.0\n",
            "Total rewards:  20.0\n",
            "Total rewards:  40.0\n",
            "Total rewards:  40.0\n",
            "Total rewards:  40.0\n",
            "Total rewards:  40.0\n",
            "Total rewards:  40.0\n",
            "Episode length  16\n",
            "EPISODE  6\n",
            "Total rewards:  0.0\n",
            "Total rewards:  0.0\n",
            "Total rewards:  0.0\n",
            "Total rewards:  0.0\n",
            "Total rewards:  20.0\n",
            "Total rewards:  20.0\n",
            "Total rewards:  20.0\n",
            "Total rewards:  20.0\n",
            "Total rewards:  20.0\n",
            "Total rewards:  20.0\n",
            "Total rewards:  20.0\n",
            "Total rewards:  20.0\n",
            "Total rewards:  20.0\n",
            "Total rewards:  20.0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}